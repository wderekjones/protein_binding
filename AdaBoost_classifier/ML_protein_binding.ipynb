{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from sklearn.preprocessing.imputation import Imputer\n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import cPickle as pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Feature Group\n",
    "group_features() allows us to select a particular group of feature i.e, protein features, ligand features or protein-ligand features.\n",
    "The allowed values for \"group_name\" are 0, 1 or 2\n",
    "- 0 - Protein Features\n",
    "- 1 - Ligand Features\n",
    "- 2 - Protein Ligand Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# all_feature_names is a list of all the features obtained from the parser output file.\n",
    "# group_name can be 0, 1 or 2.\n",
    "# 0 - Protein Features\n",
    "# 1 - Ligand Features\n",
    "# 2 - Protein-Ligand Features\n",
    "def group_features(all_feature_names, group_name):\n",
    "    if group_name == 0:\n",
    "        feature_names = pickle.load(open('data/protein_features.pkl', 'rb'))\n",
    "    elif group_name == 1:\n",
    "        feature_names = pickle.load(open('data/ligand_features.pkl', 'rb'))\n",
    "    elif group_name == 2:\n",
    "        feature_names = pickle.load(open('data/protein_ligand_features.pkl', 'rb'))\n",
    "\n",
    "    # returns the indices of all the features that belong to a specific group\n",
    "    indices = np.where(np.in1d(all_feature_names, feature_names['feat_name']))\n",
    "\n",
    "    feat_name = all_feature_names[indices]\n",
    "    return feat_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameter Tuning\n",
    "\n",
    "The AdaBoost Classifier requires us to specify the number of weak learners to be used in training the model. In order to find the optimal number of weak learners that yield the best result, I have implemented a function parameter_tuning() that trains the model with varying number of weak learners in increments of 5 and tests it on a 5 fold cross validation set. Taking into account the computing resources required for the process, I have restricted the maximum number of weak learners to 600 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parameter_tuning(X, Y):\n",
    "    \n",
    "    # base_estimators for Adaboost from 5 to 601 in increments of 5\n",
    "    estimator_list = np.arange(5, 601, 5)\n",
    "    \n",
    "    # 5 fold cross validation\n",
    "    ss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n",
    "    final_score_list = []\n",
    "    \n",
    "    for each_value in estimator_list:\n",
    "        f_score_list = []\n",
    "        for train_index, test_index in ss.split(X, Y):\n",
    "            X_train = X.iloc[train_index]\n",
    "            Y_train = Y.iloc[train_index]\n",
    "            X_test = X.iloc[test_index]\n",
    "            Y_test = Y.iloc[test_index]\n",
    "\n",
    "            adaboost = AdaBoostClassifier(n_estimators=each_value, random_state=0)\n",
    "            adaboost.fit(X_train, Y_train)\n",
    "\n",
    "            predicted_labels = adaboost.predict(X_test)\n",
    "            precision, recall, f_score, _ = precision_recall_fscore_support(Y_test, predicted_labels, average='binary')\n",
    "            \n",
    "            f_score_list.append(f_score)\n",
    "        final_score_list.append(sum(f_score_list)/len(f_score_list))\n",
    "    \n",
    "    # plot the Fscore for increasing number of base estimators. \n",
    "    plt.plot(np.arange(5,601,5), final_score_list, 'r', label='test f_score')\n",
    "    plt.xlabel('n_estimators')\n",
    "    plt.ylabel('f_score')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AdaBoost Classifier\n",
    "AdaBoost Classifier is composed of a number of weak learners that is combined using a weighted sum to produce the final output of the boosted model. For more information on AdaBoost here is a useful                              [link](http://mccormickml.com/2013/12/13/adaboost-tutorial/)\n",
    "\n",
    "feature_selection()\n",
    "- Uses Random Forest Classifier to determine the feature importances and produces the 10 most important features.\n",
    "\n",
    "preprocessing()\n",
    "- Does basic preprocessing on the data which includes accounting for missing values, imputation strategy, etc\n",
    "\n",
    "sep_cluster()\n",
    "- Used to separates individual protein clusters from the data, if needed\n",
    "- Random Sampling equal number the positive and negative samples for training\n",
    "\n",
    "k_cross_validation()\n",
    "- It is used to optimize the hyper parameters that produces the best result.\n",
    "\n",
    "adaboost()\n",
    "- The classifier is trained on the features obtained from feature selection module.\n",
    "- If the flag use_cross_val is set to 1, it tests on the cross validation set and if it is set to 0, then it tests on the blind test set.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def feature_selection(X, Y, col_names):\n",
    "    \n",
    "    rf = RandomForestRegressor(n_jobs=-1, random_state=0)\n",
    "    rf.fit(X, Y)\n",
    "    print \"Features sorted by their score:\"\n",
    "    \n",
    "    # mapping feature importances to feature names\n",
    "    features = sorted(zip(map(lambda x: round(x, 4), rf.feature_importances_), col_names), \n",
    "                 reverse=True)\n",
    "    scores, feature_names = zip(*features)\n",
    "    feature_names = list(each_name.strip() for each_name in feature_names)\n",
    "    \n",
    "    # returns the name of top 10 features \n",
    "    return feature_names[:10]\n",
    "\n",
    "def preprocessing(df_cluster):\n",
    "    df_cluster.replace(to_replace='na', value=np.nan, inplace=True)\n",
    "    imputer = Imputer()\n",
    "    \n",
    "    # imputes missing values by the mean of the respective column\n",
    "    df_cluster = pd.DataFrame(imputer.fit_transform(df_cluster), columns=df_cluster.columns)\n",
    "    return df_cluster\n",
    "\n",
    "def sep_cluster(cluster_name):\n",
    "    df_cluster = pd.read_csv('data/ml_pro_features_labels_train_final.csv')\n",
    "    if cluster_name != 'all_cluster':\n",
    "        \n",
    "        # selectes the samples based on the protein cluster number\n",
    "        df_cluster = df_cluster[df_cluster['cluster_number'] == cluster_name]\n",
    "    df_cluster = preprocessing(df_cluster)\n",
    "    df_cluster.drop(['mmgbsaEnergy', 'cluster_number'], axis=1, inplace=True)\n",
    "    \n",
    "    # shuffles the training data\n",
    "    df_cluster = df_cluster.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # selects 4000 positive samples\n",
    "    pos_df = df_cluster[df_cluster['label'] == 1][0:4000]\n",
    "    \n",
    "    # selects 4000 negative samples\n",
    "    neg_df = df_cluster[df_cluster['label'] == 0][0:4000]\n",
    "    pos_neg_comb = pd.concat([pos_df, neg_df])\n",
    "    return pos_neg_comb\n",
    "\n",
    "def k_cross_validation(X, Y, k):\n",
    "    f_score_list = []\n",
    "    acc_list = []\n",
    "    precision_list = []\n",
    "    recall_list = []\n",
    "    \n",
    "    # splits the data into k folds\n",
    "    ss = StratifiedShuffleSplit(n_splits=k, test_size=0.2, random_state=0)\n",
    "    for train_index, test_index in ss.split(X, Y):\n",
    "        X_train = X.iloc[train_index]\n",
    "        Y_train = Y.iloc[train_index]\n",
    "        X_test = X.iloc[test_index]\n",
    "        Y_test = Y.iloc[test_index]\n",
    "\n",
    "        adaboost = AdaBoostClassifier(n_estimators=1000, random_state=0)\n",
    "        adaboost.fit(X_train, Y_train)\n",
    "        predicted_labels = adaboost.predict(X_test)\n",
    "        \n",
    "        # computes the accuracy\n",
    "        acc = accuracy_score(Y_test, predicted_labels)\n",
    "        \n",
    "        # computes the precision, recall and F-score\n",
    "        precision, recall, f_score, _ = precision_recall_fscore_support(Y_test, predicted_labels, average='binary')\n",
    "        f_score_list.append(f_score)\n",
    "        acc_list.append(acc)\n",
    "        precision_list.append(precision)\n",
    "        recall_list.append(recall)\n",
    "        \n",
    "    print \"Accuracy\", sum(acc_list)/len(acc_list)\n",
    "    print \"Precision\", sum(precision_list)/len(precision_list)\n",
    "    print \"Recall\", sum(recall_list)/len(recall_list)\n",
    "    print \"fscore\", sum(f_score_list)/len(f_score_list)\n",
    "    \n",
    "def adaboost(use_cross_val):\n",
    "    pos_neg_comb = sep_cluster('all_cluster')\n",
    "    pos_neg_comb = pos_neg_comb.sample(frac=1).reset_index(drop=True)\n",
    "    Y = pos_neg_comb['label']\n",
    "    X = pos_neg_comb.drop(['label'], axis=1)\n",
    "    names = X.columns.values\n",
    "    \n",
    "    # Features identified after feature selection\n",
    "    feat_names = ['H-049', 'P_VSA_MR_8', 'F03[N-Cl]', 'F01[C-N]', 'B03[N-Cl]', 'T(N..N)', 'PCR', 'SaaN', 'SsssCH', 'Eig02_EA(dm)']\n",
    "    \n",
    "    ## to be uncommented only when a group of features is used in training the classifier\n",
    "    #protein_features = group_features(names, 1)\n",
    "    #protein_features = np.append(protein_features, ['label'])\n",
    "\n",
    "    X = X[feat_names]\n",
    "    if use_cross_val == 1:\n",
    "        k_cross_validation(X, Y, 5)\n",
    "    else:\n",
    "        test_set = pd.read_csv('data/ml_pro_features_labels_test_final.csv')\n",
    "        Y_test = test_set['label']\n",
    "        X_test = test_set.drop(['label','moleculeName'], axis=1)\n",
    "        X_test = X_test[feat_names]\n",
    "        \n",
    "        # preprocessing the test data\n",
    "        X_test = preprocessing(X_test)\n",
    "        adaboost = AdaBoostClassifier(n_estimators=1000, random_state=0)\n",
    "        adaboost.fit(X, Y)\n",
    "        predicted_labels = adaboost.predict(X_test)\n",
    "        acc = accuracy_score(Y_test, predicted_labels)\n",
    "        precision, recall, f_score, _ = precision_recall_fscore_support(Y_test, predicted_labels, average='binary')\n",
    "        print \"Accuracy\", acc\n",
    "        print \"Precision\", precision\n",
    "        print \"Recall\", recall\n",
    "        print \"fscore\", f_score\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    adaboost(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
